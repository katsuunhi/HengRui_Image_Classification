{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "import vgg16\n",
    "import utils\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取，调整大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f=pd.read_csv('.\\TrainSet\\TrainSetLabels.csv')\n",
    "ne=f.sample(frac=0.3) # 分配比例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch=()\n",
    "labels=[]\n",
    "cout=0\n",
    "for i in ne.iterrows():\n",
    "    img=Image.open(r'.\\TrainSet\\\\' +i[1]['Name'][1:-1],'r')\n",
    "    if img.mode!='RGB':\n",
    "        img = img.convert(\"RGB\")\n",
    "    img=img.resize((224,224))\n",
    "    matrix = np.array(img).reshape((1,224,224,3))\n",
    "    batch=batch + (matrix,)\n",
    "    labels.append(i[1]['Label'])\n",
    "    cout=cout+1\n",
    "    if cout==100:\n",
    "        print('d')\n",
    "        cout=0\n",
    "batch = np.concatenate(batch, 0)\n",
    "print(batch.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将图片转化为特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=None\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        vgg = vgg16.Vgg16()\n",
    "        input_ = tf.placeholder(\"float\", [None,224, 224, 3])\n",
    "        with tf.name_scope(\"content_vgg\"):\n",
    "            vgg.build(input_)\n",
    "        turn=len(batch)//64\n",
    "        for i in range(turn+1):\n",
    "            if i==turn:\n",
    "                feed_dict = {input_: batch[turn*64:]}\n",
    "            else:\n",
    "                feed_dict = {input_: batch[i*64:i*64+64]}\n",
    "            codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "            if codes is None:\n",
    "                    codes = codes_batch\n",
    "            else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('codes', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "\n",
    "with open('labels', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "with open('labels','r') as f:\n",
    "    label = csv.reader(f, delimiter = '\\n')\n",
    "    for i in label:\n",
    "        if i != []:\n",
    "            labels.append(int(i[0]))\n",
    "\n",
    "with open('codes','r') as f:\n",
    "    codes = np.fromfile(f, dtype = np.int32)\n",
    "    step = 4096\n",
    "    b = [codes[i : i + step] for i in range(0, len(codes), step)]\n",
    "    codes = np.array(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, labels))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]\n",
    "\n",
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加全连接网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入数据的维度\n",
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "# 标签数据的维度\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "# 加入一个256维的全连接的层  （这里可能要改）\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 4096)\n",
    "\n",
    "# 加入一个257维的全连接层\n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)\n",
    "#logits = tf.contrib.layers.fully_connected(inputs_, labels_vecs.shape[1], activation_fn=None)\n",
    "\n",
    "# 计算cross entropy值\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "\n",
    "# 计算损失函数\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# 采用用得最广泛的AdamOptimizer优化器\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# 得到最后的预测分布\n",
    "predicted = tf.nn.softmax(logits)\n",
    "\n",
    "# 计算准确度\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练添加的全连接层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" 这是一个生成器函数，按照n_batches的大小将数据划分了小块 \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # 如果不是最后一个batch，那么这个batch中应该有batch_size个数据\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # 否则的话，那剩余的不够batch_size的数据都凑入到一个batch中\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # 生成器语法，返回X和Y\n",
    "        yield X, Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 运行多少轮次\n",
    "epochs = 100\n",
    "# 统计训练效果的频率\n",
    "iteration = 0\n",
    "# 保存模型的保存器\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            # 训练模型\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if loss == 0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "                break\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y}\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                # 输出用验证机验证训练进度\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    # 保存模型\n",
    "    saver.save(sess, \"checkpoints/cif.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
